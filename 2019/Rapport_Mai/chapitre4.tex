\chapter{Implémentations  } \label{chapter-implementations}


\section{Introduction}

Le but de ce chapitre est de présenter les différentes implémentations du principe de détection des anomalies.  Dans un premier temps, nous présentons brièvement l'implémentation du travail de  référence. Ensuite, nous décrivons l'intégration des services Amazon S3 et Athena dans l'implémentation de référence.  De sorte à récupérer les traceroutes depuis Amazon S3 et à travers   Amazon Athena au lieu de les récupérer depuis MongoDB. Enfin, nous présentons l'implémentation détaillée du principe de détection en Spark/Scala. 

\section{Implémentation avec MongoDB}

MongoDB est la technologie Big Data utilisée par  Fontugne et al.  dans l'implémentation de l'outil de détection \cite{InternetHealthReport}. 
%MongoDB est une technologie conçue pour assurer  le stockage de données dans un processus d'analyse de données.


Dans MongoDB, les traceroutes sont organisés  dans des collections.  Par convention,  $V$  vaut $6$ s'il s'agit de l'adressage IPv6 et est vide dans le cas de l'adressage IPv4.  Le nom d'une collection est structuré comme suit: 	$tracerouteV\_YYYY\_MM\_DD$. Où $ YYYY\_MM\_DD $ représente la date du jour pendant lequel les traceroutes stockés dans la collection $$tracerouteV\_YYYY\_MM\_DD$$ sont effectués.
La nomenclature  des collections permet de ne récupérer que les traceroutes concernés par l'analyse lancée.
L'implémentation détaillée de l'outil de détection est disponible sur GitHub \cite{InternetHealthReport}.  

En faisant appel à d'autres fonctions, les deux fonctions 
\footnote{Voir l'implémentation des deux fonctions sur : \url{https://github.com/InternetHealthReport/tartiflette/blob/master/analysis/plot.py}, consulté  le $ 02/02/2018$.} principales qui permettent de tracer l'évolution du délai des liens sont  :  

\textit{\textbf{getRttData(configFile=None)}} : en se basant sur les paramètres indiqués à travers le fichier de configuration \textit{configFile}, les traceroutes sont préparés (Phase I). Cette fonction renvoie les liens identifiés avec leurs RTTs différentiels, leurs périodes et les sondes ayant identifié chaque lien.

\textit{\textbf{rttEvolution(res, ips, suffix)}} : cette fonction prend en paramètre \textit{res} qui représente les détails du lien : les périodes de l'analyse, les RTTs différentiels qui correspondent à chaque période et les sondes ayant identifié chaque RTT différentiel. Le paramètre \textit{ips} représente un lien;  deux adresses IPs. Le paramètre \textit{suffix} n'a pas été utilisé dans la fonction.


\paragraph{Notes sur l'organisation des traceroutes dans la base de données MongoDB}~

Avec MongoDB,  la manière d'organiser les données dans des collections est importante pour assurer l'optimalité de leur récupération. Dans l'implémentation de référence, chaque collection reprend les traceroutes d'une journée, peu importe leur destination. Durant  une journée d'analyse et avec une fenêtre d'une heure,  une même collection est consultée $24$ fois pour renvoyer les traceroutes qui corespondent à chaque fenêtre.   Or, si les collections ont été créées de sorte à contenir une heure de traceroutes, ceci aurait réduit la consultation des données inutilement dans le cas où la fenêtre est d'une heure. 
%mais aussi pour les fenêtres de plus d'une heure. 

De plus, en partant du fait qu'une collection reprend une journée de traceroute, le choix de la taille de la fenêtre peut amener à négliger des traceroutes. Si par exemple la fenêtre vaut   $ 4600 $ secondes et l'analyse s'étale sur $2$ journées, selon l'implémentation de référence, les traceroutes relatifs à la période entre  \textit{07/02/2018 23:00} et \textit{07/02/2018 00:15}   sont recherchés dans la collection  \textit{traceroute\_2018\_02\_07} et les traceroutes 
relatifs à la période entre \textit{08/02/2018 00:16} et \textit{08/02/2018 01:32}  sont recherchés dans la collection \textit{traceroute\_2018\_02\_08}. Dans ce cas, les traceroutes entre \textit{08/02/2018 00:00} et \textit{08/02/2018 00:15} sont négligés.

En ce qui concerne la capacité de  stockage d'une base de données MongoDB,  elle est limitée par les ressources de la machine locale où elle est installée. 
%  il existe la version cloud de MongoDB (MongoDB Atlas). 



\section{Implémentation avec Amazon S3 et Amazon Athena} \label{implementation-athena}

%\paragraph{Application sur les traceroutes}~

\paragraph{Vue générale}~

Les trois services d'Amazon (Amazon S3, Amazon Glue  et Amazon Athena) ont été combinés  afin de créer un environnement d'analyse de données massives. 
Un des scénarios possibles mettant en pratique ensemble ces trois services est illustré dans la Figure
\ref{fig:gluecrawler}\footnote{Amazon Redshift  est un entrepôt de données et  Amazon Quicksight  est un service cloud d'informatique décisionnelle.}. Les  détails concernant chaque service sont donnés dans les sections suivantes.

\begin{figure}[h]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1\linewidth]{illustrations/glue_crawler}
	\caption{Une combinaison des services web d'Amazon : Amazon S3, Amazon Glue, Amazon Athena, Amazon Quicksight  et Amazon Redshift}
	\label{fig:gluecrawler}
	\source{\url{https://docs.aws.amazon.com/fr_fr/athena/latest/ug/glue-best-practices.html}, consultée le $16/05/2018$.}
\end{figure}
Afin d'utiliser Amazon Athena pour l'interrogation des traceroutes stockés dans des fichiers, il faut d'abord  stocker ces fichiers dans Amazon S3. De plus, il faut créer un  schéma de données. Il s'agit de créer une table comme les tables dans un SGBDR. Chaque enregistrement dans cette table correspond à une ligne dans les fichiers de données censés être lus par cette table. Il existe deux manières pour créer une table dans Athena : en utilisant Amazon Glue ou en la créant  manuellement.  \textit{traceroutes\_api} désigne le nom de la table reprenant tous les traceroutes.
%Une vue globale du  processus de l'analyse  est illustré dans la Figure  




\paragraph{Création de la table traceroutes avec Amazon Glue}~

Dans le but de découvrir rapidement le schéma des traceroutes, la détection automatique du schéma a été lancées avec Amazon Glue sur un ensemble de  traceroutes enregistrés dans un fichier faisant une taille de $500$ MO. Toutefois, la détection a échoué. Autrement dit, Amazon Glue n'a pas pu inférer le schéma d'une seule table capable de lire tout traceroute dans ce fichier.  L'échec de l'inférence est dû au fait que le fichier contient des traceroutes différents en terme de structure, car la structure dépend de la version du firmware de la sonde ayant effectué le traceroute. Les différentes versions du firmware  pour chaque type de mesure sont détaillées dans le site Web d'Atlas\footnote{URL : \url{https://atlas.ripe.net/docs/data_struct/}, consultée le $16/01/2018$.}.
%L'origine de cette différence  est le fait que ces traceroutes ont été effectués par des sondes ayant un firmware différent. Car le contenu des résultats d'une requête traceroute  et son organisation dans un objet JSON dépend partiellement du firmware de la sonde. 

\paragraph{Création manuelle de la table \textit{traceroutes\_api}}~ \label{creer-table-traceroute}

Suite à l'échec d'Amazon Glue  et comme la structure des réponses traceroutes est disponible,  la structure de la table a été créée   manuellement en se basant sur la structure détaillée d'une réponse traceroute pour chaque version du firmware.  Les différentes structures de réponses d'une requête traceroute n'ont posé aucun problème dans la création manuelle de la table. Dans notre cas, la réussite de la création manuelle est due au fait que les attributs dont l'outil de détection a besoin sont présents dans toutes les versions du firmware d'une part. D'autre part, Amazon Athena est flexible en ce qui concerne l'association entre un objet JSON et un enregistrement dans une table. Autrement dit, si un attribut existe dans l'objet JSON, la colonne correspondante prend sa valeur. Dans le cas échéant, la colonne reste vide.  %\subparagraph{Création de la table traceroutes } 
La création d'une table dans Amazon Athena  reprend les parties suivantes  :
\begin{itemize}
	\item les colonnes de la table avec le type correspondant (int, string, array pour définir une liste, struct pour définir un objet );
	\item LOCATION : c'est l'endroit où les données sont stockées dans Amazon S3, il faut préciser le chemin vers le compartiment de données;
	\item ROW FORMAT SERDE : elle définit la manière dont chaque ligne d'un fichier de données est sérialisée/désérialisée par Amazon Athena;
	\item PARTITIONED BY : elle définit la manière dont les données sont organisées dans le compartiment de données; 
	\item WITH serdeproperties : elle définit les options de la sérialisation/désérialisation.
\end{itemize}

La table \textit{traceroutes\_api}  créée est présentée dans le Listing \ref{createAthenaTable}.
\begin{lstlisting}[language=SQL, basicstyle=\footnotesize, label=createAthenaTable, caption={Création de la table des traceroutes dans Amazon Athena }]
CREATE EXTERNAL TABLE traceroutes_api(
af int,
bundle int,
dst_addr string,
dst_name string,
fw int,
endtime int,
`from` string,
group_id int,
lts int,
msm_id int,
msm_name string,
paris_id int,
prb_id int,
proto string,
size int,
src_addr string,
`timestamp` int,
ttr float,
type string,
result array< struct< hop:int,error:string, result:array<
struct<x:string, err:string, `from`:string, ittl:int, edst:string, late:int, mtu:int, rtt:float, size:int, ttl:int , flags:string, dstoptsize:int, hbhoptsize:int, icmpext:
struct<version:int, rfc4884:int, obj:array< 
struct<class:int, type:int, mpls:array<struct< exp:int, label:int, s:int, ttl:int>>>>>>>>> 
)
PARTITIONED BY (
af_ string,
type_ string,
msm string ,
year string,
month string,
day string,
hour string
) 
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
WITH serdeproperties ('paths'='af,bundle,dst_addr, dst_name,fw, endtime, from, lts, msm_id, paris_id, prb_id, proto, size, src_addr, timestamp, type,fw, msm_name' ) 
LOCATION 's3://ripeatlasdata/traceroute/source=api/'
\end{lstlisting}





\paragraph{Partitionnement des données stockées dans Amazon S3}~

La table \textit{traceroutes\_api} créée a été conçue de sorte qu'elle prenne en compte  le partitionnement de données dans un compartiment S3.  L'utilisation du partitionnement est optionnelle. Toutefois, il permet de limiter la quantité de données à analyser par une requête Amazon Athena. Le partitionnement améliore donc les performances d'Amazon Athena. D'une part, la requête s'exécute plus rapidement. D'autre part, ceci réduit les coûts engendrés  suite à l'utilisation d'Amazon Athena, car ce dernier est facturé selon la quantité de données analysées. En pratique,  une partition créée joue un rôle similaire à celui d'une colonne durant l'interrogation d'une table dans Athena. 

Prenons un exemple illustrant l'apport du partitionnement. Nous avons des traceroutes effectués en adressage IP  version  $ 4 $ et $ 6 $.


\textit{af\_} désigne le type d'adressage : \textit{af\_} vaut $4$ en cas d'adressage IPv4 et $6$ en cas d'adressage IPv6. Sans l'utilisation du partitionnement et si on ne souhaite récupérer que  les traceroutes ayant comme adressage IPv4, tous les traceroutes présents dans le compartiment S3 (appelé \textit{s3://ripeatlasdata}) sont évalués.
%\footnote{L'évaluation du type  d'adressage est effectué selon la valeur de l'attribut \textit{af} d'un traceroute, il vaut \textit{4} ou \textit{6}.}.

Toutefois, en partitionnant les données suivant, par exemple, le type d'adressage, seuls les fichiers dans la partition
%\footnote{Partition dans le sens d'Amazon Athena.} 
af\_ = $4$  sont analysés. Par conséquent, le partitionnement permet de réduire les coûts d'utilisation du service Amazon Athena, surtout si la quantité de données est très importante. 


Les partitions   créées sont illustrées  dans la Figure 	\ref{fig:partitionnement-athenaa}. Ces partitions ont été créées manuellement. Le choix des partitions à créer  dépend de l'interrogation des données; des paramètres de la requête SQL conçue pour récupérer  les données. Par exemple, si toutes les requêtes SQL  sont indifférentes concernant la version IP, il n'est pas nécessaire de créer la partition af\_. Pour précision, le partitionnement permet  d'organiser les données en une structure de répertoires hiérarchique. Les valeurs de chaque partition sont distinctes.
Le  Tableau \ref{tab:partition-description} 
reprend les  partitions créées dans le compartiment S3 reprenant les traceroutes à analyser.  La première colonne représente le nom de la partition, dans 
la deuxième colonne décrit quelques valeurs de chaque partition et la troisième colonne de ce tableau reprend la description de la partition.

\begin{figure}[h]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1\linewidth]{illustrations/partitionnement-athena}
	\caption{L'organisation des traceroutes dans le compartiment Amazon S3 \textit{s3://ripeatlasdata}}
	\label{fig:partitionnement-athenaa}
\end{figure}

\begin{table}[h]
	\centering
	\captionsetup{justification=centering}
	\begin{tabular}{|c|c|l|}
		\hline 
		\textbf{partition}	& \textbf{Valeurs} & \multicolumn{1}{c|}{\textbf{Commentaires} }\\ 
		\hline 
		source& api & Les  traceroutes récupérés depuis le dépôt  d'Atlas via l'API \\ 
		\cline{2-3}
		&typeanddate& Les  traceroutes récupérés depuis la page Web\\
		\hline 
		af\_& 4  & Les traceroutes en adressage IPv4 \\ 
		\cline{2-3} &6& Les traceroutes en adressage IPv6\\	\hline 
		type& builtin  & Les traceroutes en provenances des mesures intégrées \\ 
		\cline{2-3} 
		&anchor& Les traceroutes à destinations des ancres\\ \hline
		msm& 5001 & Les traceroutes ayant msm\_id = 5001 \\ 
		\cline{2-3}  &5004& Les traceroutes ayant msm\_id = 5001 \\
		\hline 
		year& 2016 & Les traceroutes effectués en $2016$ \\ 
		\hline 
		month& 10 & Les traceroutes effectués en octobre \\ 
		\hline 
		day& 1 & Les traceroutes effectué le premier du mois \\ 
		\hline 
	\end{tabular}
	\caption{Exemple des partitions créées dans un compartiment Amazon S3} 
	\label{tab:partition-description}
\end{table}

Les   partitions \textit{af\_} et \textit{type\_} sont nommées de cette manière, au lieu de \textit{af} et \textit{type} car  la table \textit{traceroutes\_api} contient des colonnes ayant ces noms et comme les partitions agissent comme des colonnes  lors de l'évaluation d'une requête avec Amazon Athena, les noms de ces partitions ont été adaptés.

Prenons un exemple, les traceroutes qui se trouvent dans  le fichier\\
\textit{2016-10-01 00:00:00\_msmId5001.json.gz} sont analysés par toute requête SQL Athena  
impliquant les partitions d'une des manières énumérées dans le Tableau \ref{partitions-sql}. En effet, d'une combinaison à une autre, le volume de données consultées qui est différent. 

\begin{table}[H]
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l}
			(source = api) \\ \hline
			(source = api et af\_ = 4)\\ \hline
			(source = api et af\_ = 4 et type = builtin) \\ \hline
			(source = api et af\_ = 4 et type = builtin et msm = 5001) \\ \hline
			(source = api et af\_ = 4 et type = builtin et msm = 5001 et year = 2016) \\ \hline
			(source = api et af\_ = 4 et type = builtin et msm = 5001 et year = 2016 et month = 10) \\\hline
			(source = api et af\_ = 4 et type = builtin et msm = 5001 et year = 2016 et month = 10 et day = 1)\\ \hline
		\end{tabular}
	}
	\caption{Exemple d'utilisation des partitions dans une requête SQL dans Amazon Athena}
	\label{partitions-sql}
\end{table}

\paragraph{Interrogation des données Avec Amazon Athena}~  \label{sql-athena-request}

Une fois les fichiers de données  synchronisés vers le compartiment AWS S3 et le schéma  de données  créé, nous  passons à l'interrogation de données en utilisant les requêtes SQL basées sur Presto.  Une requete SQL dans Athena est décomposée suivant trois parties principales:

	\textbf{les données sollicitées} : préciser  la  colonne souhaitée ou  bien une colonne transformée suite à l'application d'une ou de plusieurs fonctions de Presto;
	
    \textbf{les partitions de données concernées} : la requête SQL est paramétrée de sorte à limiter les données à analyser sur Amazon S3;
	
	\textbf{les paramètres appliqués sur les colonnes} : la requête SQL doit filtrer les données suivant les conditions sur les colonnes de la table.


La Figure \ref{fig:sqlrequestathena} montre  un exemple d'une  requête SQL dans Athena.  Cette requête fait appel aux fonctionnalités  de Presto\footnote{URL : \url{https://prestodb.github.io/docs/current/}, consulté le $10/06/2018$.} comme  \textit{transform}, \textit{concat}, etc. 

Par les lignes $1$ à $2$, nous définissons  une relation nommée appelée \textit{dataset}.
Pour les données sollicitées, ce sont les trois colonnes \textit{prb\_id}, \textit{from}, \textit{msm\_id} (ligne $2$) et la liste de saut (\textit{hops}) (ligne $9$)  obtenue après quelques vérifications (lignes $3$ à $8$). L'objectif de ces lignes est de vérifier la validité du traceroute.  Le premier \textit{result} contient la liste sauts, pour chaque saut, \textit{result} représente la liste des signaux. D'abord nous vérifions la validité d'un traceroute, ensuite nous transformons chaque signal (\textit{entry}) de la liste des signaux de chaque saut (\textit{x}) afin de ne récupérer que \textit{from} et \textit{rtt} entourés par "".  Ceci  facilite le passage d'une réponse fournie par Amazon Athena à une structure de données manipulée en Python.

 A la ligne $10$, nous indiquons  la table des  traceroutes (voir les détails de la table dans \ref{createAthenaTable}). La ligne $11$ permet de filtrer les traceroutes selon leur timestamp.
 Les traceroutes à analyser sont ceux obtenus en vérifiant les conditions dans la ligne  $12$ et $13$. C'est à dire, Athena va regarder les traceroutes qui se trouvent dans les sous dossiers year=2016/month=10/day=21 qui se trouvent  dans les deux dossiers msm=5001 et msm=5004. Ce que revient à chercher les traceroutes dans les deux endroits suivants:

\begin{lstlisting}[basicstyle= \footnotesize]
s3://ripeatlasdata/traceroute/source=api/af_=4/type_=builtin/msm=5001/year=2016/month=10/day=21
s3://ripeatlasdata/traceroute/source=api/af_=4/type_=builtin/msm=5004/year=2016/month=10/day=21
\end{lstlisting}

Enfin, les lignes $15$ à $18$  indiquent les  colonnes  sélectionnées.
Le choix des partitions est basé sur les identifiants des mesures indiquées dans les paramètres. Si aucun des identifiants n'est précisé, toutes les identifiants des mesures sont considérés. En ce qui concerne les partitions \textit{year}, \textit{month} et \textit{day}, leur valeur est inférée de la période en cours.

\begin{figure}[h]
	%	\resizebox{}{!}{
	\centering
	\includegraphics[width=1\linewidth]{illustrations/sqlRequestAthena2.png}
	\caption{Une exemple d'une requête SQL sur Amazon Athena}
	\label{fig:sqlrequestathena}
\end{figure}




\begin{comment}

\begin{table}[h]
	\begin{tabularx}{\linewidth}{lX}
     \textbf{Lignes}& \textbf{Commentaires}\\ \hline
     1 à 2 & : la clause  \textit{with} définit les relations nommées à utiliser dans une requête. \\  \hline
     3 à 8 & : vérifier la validité du traceroute.  Le premier \textit{result} contient la liste sauts, pour chaque saut, \textit{result} représente la liste des signaux. D'abord on vérifie la validité d'un traceroute, ensuite on transforme chaque signal (\textit{entry}) de la liste des signaux de chaque saut (\textit{x}) afin de ne récupérer que \textit{from} et \textit{rtt}. Les derniers sont entourés par "" pour faciliter le passage d'une réponse fournie par Amazon Athena à une structure de données manipulée en Python.   \\  \hline
     9 & : la colonne qui représente les sauts obtenus prennent le nom \textit{hops}.\\  \hline
     10 & indiquer la table Athena  contenant les traceroutes.\\  \hline
     11& : préciser la période concernée  \\  \hline
     12  à 13 & : préciser les partitions de données à consulter \\   \hline
     15 à 19 & : sélectionner les colonnes utiles. \\  \hline
	\end{tabularx}
\end{table}

\end{comment}
\paragraph{Intégration d'Amazon Athena dans l'outil de détection}~ \label{integration-aws-possibilite-une}


Pour intégrer Amazon Athena dans l'outil de détection \cite{InternetHealthReport}, on distingue deux possibilités. La première possibilité n'utilise Athena que pour récupérer  les traceroutes stockés dans Amazon S3 vers  la machine locale. Ensuite, cette dernière poursuit les traitements  décrits dans la phase I et II. Dans ce cas, nous ne profitons pas  des performances d'Amazon Athena vu que les traitements complexes sont effectués dans la machine locale.
%vérifiés en terme de validité; c'est l'objectif des étapes $1$ et $2$ dans le processus de la création de l'évolution des RTTs différentiels des liens (voir la section \ref{steps-rtt-analysis}).  
%Les traitements qui suivent (étapes  à partir de $3$) sont effectués dans la machine locale. Dans ce cas, l'utilisation des technologies  Big Data est limité qu'au niveau stockage de données massives. 

La deuxième possibilité vise la maximisation des traitements des  phases I et II au sein de l'infrastructure  d'Athena. De ce fait, la machine locale n'a qu'à recevoir les derniers résultats de la détection, voire les résultats finaux. Pour cette deuxième possibilité, les données doivent être manipulées de sorte à maximiser,  	au niveau d'Amazon Athena,  les traitements relatifs à toutes les étapes des  phases I et II. 

Pour la deuxième possibilité, le défi est de trouver la requête ou bien l'ensemble de requêtes SQL à exécuter sur Athena en vue d'avoir l'évolution du RTT différentiel des liens. 
Vu la complexité des  étapes des phases I et II, on ne peut pas trouver une seule requête SQL assurant toutes ces étapes à la fois. Supposons qu'il existe une requête SQL capable de trouver les liens possibles avec leurs RTTs différentiels : à l'étape $ 4 $ dans \ref{steps-rtt-analysis}, on construit la distribution des RTTs différentiels pour tout lien $l$ identifié dans les traceroutes de la période $d_k$. Cette distribution est mise à jour à chaque fois $l$ est identifié dans un des traceroutes  de la période $d_k$. 

Soient  $T_k$ = \{$t_{k, j}$\}  l'ensemble de traceroutes effectués durant $d_k$, avec $j \in [1, R_k]$ et $R_k$ est le nombre de traceroutes effectués durant $d_k$. Nous décrivons le parcours des traceroutes d'une période $d_k$ brièvement dans le pseudo-code \ref{alo-inference-link}. Nous n'avons pas donné  tous les détails, car l'objectif est d'évaluer la pertinence d'Athena au traitement souhaité.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\ForAll{ $t_{k, j}$ $\in$ $T_k$} \
		\State $links$ $\leftarrow$ getLinksFromTraceroute($t_{k, j}$)
		\ForAll{$l$ $\in$ $links$}
		\State updateLinkRttDistribution($l$) \label{update-link}
		\EndFor
		\EndFor
	\end{algorithmic}
	\caption{Une partie de l'étape $4$ du processus de la détection des anomalies des délais }
	\label{alo-inference-link}
\end{algorithm}

Avec : 
\begin{itemize}
	\item \textit{getLinksFromTraceroute($t_{k, j}$)} énumère tous les liens possibles dans le traceroute $t_{k, j}$.
	
	\item \textit{updateLinkRttDistribution($l$)} ajoute le RTT différentiel calculé du lien $l$ à la distribution des RTTs différentiels courante de ce lien pour la période $d_k$.
\end{itemize}


Le service Athena est conçu pour la lecture de données, toute mise à jour de données n'est pas possible avec ce service. C'est pourquoi la distribution des RTTs différentiels de chaque  lien identifié doit être sauvegardée dans un endroit accessible en lecture et en écriture, par exemple dans un compartiment AWS S3. Que ce soit un fichier reprenant la distribution des RTTs différentiels  par un seul lien ou bien un fichier pour tous les liens,   à la ligne  \ref{update-link} du pseudo-code \ref{alo-inference-link}, un fichier doit être lu et mis à jour avec de nouvelles valeurs. Pour une période $d_k$ d'une heure, le nombre de traceroutes est de l'ordre de milliers. Chaque traceroute $t_{k,j}$ peut inclure $L_{k,j}$ liens. Dans ce cas, le nombre total, d'une période $d_k$, de mise à jour de la distribution des RTTs différentiels est    $ \sum_{m=1}^{R_k}  L_{k,m}$. $ L_{k,j} $ dépend du nombre de saut du  traceroute $t_{k,j}$.

%de l'ordre $R_k$\texttimes$L$ de fois.  Cette estimation est à titre indicatif, de plus elle ne concerne que l'étape $4$, le nombre de lectures et/ou d'écritures dépend des requêtes SQL créées pour les autres étapes. 

En plus du nombre de lectures et d'écritures, relatives à la phase I, que nous venons de décrire, à la phase II, la détection des anomalies s'effectue en  comparant les intervalles de confiance. Cette comparaison révèle deux contraintes. La première contrainte concerne  la fonction permettant de calculer les deux bornes de l'intervalle de confiance de Wilson ne fait pas partie des fonctions disponibles sur Amazon Athena. D'autre part, Amazon Athena ne permet pas la création des fonctions personnalisées pour répondre à des besoins non couverts par Amazon Athena. La deuxième contrainte concerne la mise à jour de l'intervalle de confiance de référence qui doit être faite à chaque nouvelle période.


\paragraph{Evaluation des critères pour Amazon S3 et Amazon Athena  }~

Afin d'utiliser le service Amazon Athena à moindre coût, il est conseillé d'utiliser le partitionnement, car moins de frais sont appliqués. Si un partitionnement particulier est adopté, la création du schéma de données est basé sur ce partitionnement ainsi que les requêtes SQL destinés à l'interrogation de la table de données.

En ce qui concerne l'évolutivité d'une application basée sur ces deux services d'Amazon, on note que toute mise à jour de la structure de données des objets traceroutes peut affecter l'entièreté de la configuration initiale. A savoir, l'organisation des fichiers de données via le partitionnement, le schéma de données et les requêtes SQL.

Quant à la flexibilité du schéma de données, le service  Amazon Athena est tolérant aux données manquantes. Etant donné que la structure d'un objet traceroute dépend de la version du firmware de la sonde, nous avons créé trois schémas de tables. La première table  modélise tout objet traceroute de  version $5$, la deuxième modélise tout objet traceroute de version $6$ et enfin la troisième table modélise ceux ayant la version $7$. En expérimentant différentes requêtes, nous avons conclu  que Amazon Athena a pu récupérer les données de la version récente ($7$) via le schéma de la version $5$ malgré que la version $7$ a plus d'attributs par rapport à la version $5$.




\section{Implémentation  en Spark/Scala} \label{application:spark}

\subsection{Présentation de Scala} \label{scala-presentation}

%Nous allons présenter le langage Scala\footnote{Présentation basée sur \url{http://igm.univ-mlv.fr/~dr/XPOSE2011/le_langage_scala/}, consultée le $09/04/2019$}. 

La première version du langage Scala date de l'année $2003$, le terme  Scala  est dérivé de \textit{Scalable language}. 
La particularité de ce langage est le fait de proposer à la fois plusieurs avantages. Scala appartient à la 
programmation multi-paradigme, c'est  un langage à la fois orienté objet, il supporte la programmation fonctionnelle et fait partie des langages impératifs. En terme de compatibilité, Scala est interopérable avec  Java, car ce dernier est compilé en bytecode Java. Ceci permet d'utiliser du code Java avec le code Scala, de garantir l'indépendance des systèmes d'exploitation et d'utiliser la richesse des bibliothèques en Java. De plus, le code écrit en Scala est facile à lire.
Nous comparons dans le Listing \ref{java-vs-scala} l'écriture d'un même traitement écrit en Java et en Scala.
\begin{center}
	\begin{minipage}[t]{.50\textwidth}
\begin{lstlisting}[language=java, basicstyle=\small]
//Java
Integer[] intArray = {1,2,3,4,5};
List <Integer> numsList = Arrays.asList(intArray);
List <Integer> positves = new LinkedList < Integer > ();
for (int i: numsList) {
  if (i > 0) positives.add(n);
}
\end{lstlisting}
	\end{minipage}\hfill
	\begin{minipage}[t]{.49\textwidth}
\begin{lstlisting}[language=scala, basicstyle=\small]
// Scala 
val positives = List(1,2,3,4,5).filter(_ > 0)
\end{lstlisting}
	\end{minipage}
	\captionof{lstlisting}{Comparaison entre un traitement écrit en Java et en Scala}
	\label{java-vs-scala}
\end{center}
%L'implémentation proposée implique plusieurs éléments relatifs au langage Scala comme les case class, les fonctions et le RDD relatif au Spark. 

\paragraph{Notations relatives au langage Scala}~

Il existe plusieurs manières pour définir une fonction en Scala. Nous présentons celle utilisée dans les Listings du présent chapitre. Dans le Listing \ref{lst:generalFunction}, on crée une fonction appelée  \textit{functionName}, ayant deux paramètres : \textit{x} de type \textit{Int} et \textit{y} de type \textit{Int}, elle renvoie \textit{result} de type \textit{String}, le mot-clé \textit{return} est optionnel.  
\begin{lstlisting}[language=scala,firstnumber=1, caption={Exemple d'une fonction en  Scala}, label={lst:generalFunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1,numberstyle=\scriptsize]
def functionName(x: Type, y Int): String = {
	// some  code that uses x and y and return result
	return result 
}
\end{lstlisting} \par

Nous présentons  des notations   propres au langage Scala. Ce  sont les mots-clés utilisés  dans les Listings du présent chapitre. \par
\textbf{\textit{case class}} (mot-clé): définir une  classe.\par
\textit{\textbf{Seq}} (type): créer une séquence, équivalent à créer une liste. Par exemple Seq[String] représente le type d'une liste dont les éléments sont de type String. Lors du parcours d'une séquence, l'élément courant est accessible via " \_".  \par
\textbf{\textit{Dataset}} (type):  est une collection distribuée de données. Elle est similaire à un RDD (voir la section \ref{rdd-presentation}) et plus récente que les RDDs. \par
\textit{\textbf{map}} (fonction): permet de transformer le contenu d'une liste en appelant une fonction sur chaque élément de la liste, elle renvoie une liste transformée. \par
\textit{\textbf{Option}} (type) : représente une valeur optionnelle.  \par
\textit{\textbf{groupBy}} (fonction):  est une fonction utilisée pour regrouper les éléments d'une liste (\textit{l\_0}) ayant une clé en commun (\textit{key}). Cette fonction renvoie une liste de tuples. Chaque tuple (\textit{tup})  contient deux éléments. Le premier représente la clé (\textit{key})  du groupement, son contenu est accessible via \textit{tup.\_1}. Le deuxième élément est une liste d'éléments de \textit{l\_0} ayant la même clé, son contenu est accessible via \textit{tup.\_2}.\par
\textbf{\textit{List}} (type): créer une liste d'un type donné. La liste \textit{l\_1} de deux chaînes de caractères est créée comme suit:
\begin{lstlisting}[language=scala,firstnumber=1, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
l_1: List[Int] = List("aa", "bb")
\end{lstlisting} \par
\noindent \textbf{\textit{filter}} (fonction): cette fonction permet de filtrer les éléments d'une collection pour créer une nouvelle collection contenant uniquement les éléments de la collection   vérifiant une condition. Prenons un  exemple : 
\begin{lstlisting}[language=scala,firstnumber=1, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
//créer une liste d'entiers entre 1 et 10, l_2 vaut List(1, 2, 3, 4, 5, 6, 7, 8, 9)
val l_2 = List.range(1, 10)   
// filter les éléments de l_2 afin de ne garder que ceux paires, evens vaut List(2, 4, 6, 8)
val evens = l_2.filter(_ % 2 == 0)  
\end{lstlisting} \par

\textit{\textbf{collect}} (fonction): c'est une fonction liée au fonctionnement de Spark plutôt qu'au Scala, précisément, elle est appelée à partir d'un RDD.    La fonction \textit{collect} ramène la totalité des données traitées par les \textit{workers} vers le driver.\par

\textit{\textbf{flatten}} (fonction): la méthode \textit{flatten}  prend une liste de listes et concatène toutes les listes d'éléments de la liste principale afin de retourner une seule liste reprenant tous les éléments. 
%Autrement dit, la fonction \textit{flatten} prend A[A[T]] et retourne A[T], T est type de données et A peut être List, Seq, etc.

\subsection{La programmation fonctionnelle et le Big Data}
%\subsection{Les apports de la programmation fonctionnelle}

La programmation fonctionnelle est un style de programmation où l'évaluation se base sur les fonctions.  La programmation fonctionnelle se caractérise par les éléments suivants \cite{bigdatafunctional} :

\textbf{Fonctions d'ordre supérieur} ou  \textit{higher-order} functions en anglais. Dans un langage, si une fonction est traitée comme first-class value, elle est considérée comme fonction d'ordre supérieur \cite{DBLP:journals/csur/Hudak89}.
\begin{tcolorbox}
	\textbf{First-class functions}: ou fonctions de première classe dans un langage de programmation sont des fonctions qui peuvent être stockées dans des structures de données, peuvent être passées comme arguments aux fonctions et être retournées comme résultats.
\end{tcolorbox}
\par
\textbf{Structures de données immuables.} Un objet est immuable est celui qu'on ne peut pas changer son état après sa création. \par
\textbf{Stateless} ou \textit{no side effect} en anglais.  Une fonction est dite n'ayant pas  l'effet de bord, quand cette fonction ne modifie pas une variable en dehors de son environnement local. Par exemple, les fonctions qui modifient l'état d'une variable passée à cette fonction par référence sont considérées comme ayant un effet de bord.\par
%\subsection{La programmation fonctionnelle au service du Big Data }

Quand il s'agit des traitements appliqués sur des données massives dépassant la capacité d'une seule machine, la distribution de ces traitements sur plus d'une machine  est nécessaire. La programmation fonctionnelle peut apporter des facilités aux projets Big Data. 
En particulier, l'absence d'effet de bord lors de la conception des fonction est un élément clé pour faciliter la parallélisation des traitements appliqués sur des volumes importants de données. Ceci permet aussi de mieux gérer les opérations concurrentielles. 
La manipulation des structures de données immuables permet d'écrire un code lisible grâce au nombre réduit de dépendances sur ces structures. De plus, les structures de données immuables facilitent la parallélisation, car elles supportent seulement le mode lecture-seule.

%On distingue deux sous-catégories de langages de programmation fonctionnelle. Des langages qui sont purement fonctionnels et les autres hybrides.
%La programmation fonctionnelle offre de nombreux avantages dans un contexte Big Data. 



\subsection{Implémentation détaillée }
\subsubsection{Description de l'environnement}
%Spark est destiné aux traitements distribués sur un cluster de machines, toutefois,n
La création d'une application Spark/Scala implique  les étapes suivantes:
\begin{itemize}
	\item gérer les dépendances nécessaires au fonctionnement de l'application avec le fichier de modèle objet du projet (POM);
	\item écrire l'application en Scala;
	\item créer le fichier JAR de l'application Spark;
	 \item soumettre  l'application  au \textit{cluster} de machines. 
\end{itemize}

L'implémentation proposée est adaptée au mode local de Spark  ainsi qu'au  mode \textit{cluster}. 
Le code source, qui traduit l'ensemble des traitements, est organisé dans une archive de type JAR.
En ce qui concerne  l'automatisation et la gestion du fichier JAR, nous avons utilisé l'outil \textit{Maven}\footnote{URL : \url{https://maven.apache.org/}, consulté le $09/04/2019$.}.
% Les traitements de données sont organisés dans le fichier JAR.


\subsubsection{Brève présentation  de l'implémentation}

%Un programme Spark implique un ensemble d'éléments. 
%et ce à travers  un objet \textit{SparkConf}. Ce dernier contient les informations sur l'application. 
La première chose à faire lors de la conception d'une application Spark est de configurer cette dernière. Ensuite, nous utilisons en particulier le module  Spark SQL  du Spark Unified Stack (voir la section \ref{Spark Uniffied-Stack}) pour lire les traceroutes présents dans les fichiers indiquées à l'entrée de l'application. Cela permet de créer un Dataset d'objet Traceroute.  Nous convertissons ensuite un Dataset en un RDD afin d'appliquer différentes transformations aboutissant à l'identification des anomalies dans les délais des liens.

\subsubsection{Création d'une application Spark/Scala}

\paragraph{Gestion des dépendances}~

Le fichier POM permettant de gérer les dépendances est disponible sur GitHub\footnote{URL : \url{https://github.com/hayatbellafkih/SparkSalacaTraceroutesAnalysis/blob/master/rttDelaysSparkScala/pom.xml}, consultée le $23/04/2019$.}. Ces dépendances concernent les composantes de Spark comme spark-core, spark-mllib, spark-sql. A ces dépendances, ils s'ajoutent celles permettant de gérer les fichiers de type JSON. 

\paragraph{Paramètres de l'analyse}~

Afin de tracer l'évolution du délai des liens, nous avons besoin des fichiers stockant les  traceroutes  dans  des objets JSON\footnote{Voir la liste des traceroutes utilisés dans l'exemple illustratif disponible sur GitHub. URL : \url{https://github.com/hayatbellafkih/SparkSalacaTraceroutesAnalysis/blob/master/rttDelaysSparkScala/src/main/resources/test/result_modified.json}, consulté le $23/04/2019$.}, la date du début de l'analyse (p. ex. $ 1517961600 $), la date de la fin (p. ex. $ 1518134400 $) et enfin la durée d'une période (p. ex. $3600$s). En ce qui concerne les fichiers de données, en mode local, ils sont stockés localement et le chemin vers ces derniers est configuré dans un fichier de configuration. Pour le mode \textit{cluster}, le chemin vers les fichiers de traceroutes est celui vers le compartiment S3 contenant ces derniers.

\paragraph{Configuration d'une application  Spark}~

Une application Spark nécessite l'ajustement de quelques paramètres, qu'il s'agisse d'une application qui tourne en mode local ou bien en mode \textit{cluster}. 
 On peut ajuster ces paramètres selon trois possibilités. La première possibilité est à travers l'objet \textit{SparkConf} comme illustré dans l'exemple du Listing \ref{lst:label}, où nous donnons un nom à l'application Spark (ligne \ref{line:app-name}) et nous précisons l'URL du \textit{cluster} (ligne \ref{line:app-thread}). 

\begin{lstlisting}[language=scala,firstnumber=1, caption={Exemple de la configuration d'une application Spark via l'objet SparkConf},label={lst:label}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1,numberstyle=\scriptsize]
//imports
import org.apache.spark.SparkConf

// Spark configuration : create configuration
val conf = new SparkConf().setAppName("Link delay analysis") |\label{line:app-name}|
                          .setMaster("local"),  |\label{line:app-thread}|
\end{lstlisting}

Nous pouvons passer certains paramètres lors de la soumission de l'application au \textit{cluster}.  Un exemple de ces paramètres est illustré dans le Listing \ref{lst:submit}. Enfin, quelques paramètres peuvent être lus depuis le fichier de configuration  \textit{conf/spark-defaults.conf}\footnote{Plus de détails relatifs à la configuration sont disponibles sur l'URL  \url{https://spark.apache.org/docs/latest/configuration.html}, consulté le $14/04/2019$.}. 
%Certains paramètres peuvent être précisés 

%en ligne de commande ou bien dans le fichier de configuration \footnote{Plus de détails sont disponibles.}. 
%La configuration de l'application s'effectue à travers un . Il existe un nombre de  paramètres à  ajuster comme le nombre de threads à utiliser au cas où l'application Spark s'exécute en mode local.




\paragraph{Point d'entrée vers les fonctionnalités de Spark}~

Le point d'entrée vers les fonctionnalités de Spark se fait par la création du  \textit{SparkContext}. 
Néanmoins, il existe d'autres points d'entrée qui sont plus spécifiques aux composantes du \textit{Spark Unified Stack}. Par exemple,  \textit{SparkSession}  est le point d'entrée vers Spark SQL, \textit{StreamingContext} est le point d'entrée vers Spark Streaming, etc.
Dans notre cas, nous avons utilisé \textit{SparkSession} pour lire les traceroutes en tant que liste d'objets de type Traceroute.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Creation d'une session Spark},label={lst:SparkSession-initiation}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val spark = SparkSession
		.builder()
		.config(conf)
		.getOrCreate()
\end{lstlisting}

\paragraph{Lecture de données}~

L'outil de détection proposé par  Fontugne et al. n'exploite qu'une partie des données d'une réponse traceroute\footnote{Un exemple d'une réponse  traceroute  est donné dans l'annexe \ref{exemple-traceroute}.}.
En particulier, on peut utiliser Spark SQL pour  ne lire que les données qui nous intéressent, c'est un des avantages du principe du \textit{Schema-On-Read} décrit dans la section \ref{sec:schema-read-write}. 

Chaque réponse traceroute est structurée dans un objet JSON dans une seule ligne. Afin de lire chaque ligne, nous avons créé la classe Traceroute, cette dernière  a pour objectif de faire l'association entre l'objet JSON  et un objet Traceroute de sorte à encapsuler les données d'un objet JSON. La classe \textit{Traceroute} reprend le nom de la destination de la requête traceroute (\textit{dst\_name}), l'adresse IP de la sonde effectuant la requête traceroute (\textit{from}), l'identifiant de cette sonde (\textit{prb\_id}), le temps de la requête (\textit{timestamp}) et enfin la liste des sauts (\textit{Seq[Hop]}). La classe Traceroute est définie dans le Listing \ref{lst:case-class-Traceroute}.


\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la  classe Traceroute},label={lst:case-class-Traceroute}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class Traceroute(
	dst_name:  String,
	from:      String,
	prb_id:    BigInt,
	msm_id:    BigInt,
	timestamp: BigInt,
	result:    Seq[Hop])
\end{lstlisting}

Un saut  représente un des routeurs parcourus avant d'atteindre la destination finale. Nous modélisons un saut  par la classe \textit{Hop} (voir le Listing \ref{lst:case-class-hop}). Un saut  est défini par son rang (\textit{hop}), ce dernier indique l'ordre du saut en question. Etant donné que la sonde reçoit au moins trois  signaux pour chaque saut. Un saut est donc défini par une liste de signaux (\textit{Seq[Signal]}).
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la  classe Hop},label={lst:case-class-hop}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class Hop(
	var result: Seq[Signal],
	hop:        Int)
\end{lstlisting}

Un signal (\textit{Signal}) est émis par  un routeur  dont l'adresse IP est \textit{from}. Le temps nécessaire à la réception du signal par la source (ici c'est la sonde) est   \textit{rtt}. Enfin, \textit{x} est un indicateur de la validité du signal, car il se peut que la sonde ne reçoive pas une réponse d'un ou de plusieurs routeurs. Si le signal est valide, \textit{x} est une chaîne vide et \textit{rtt} et \textit{from} sont présents et ont des valeurs. Dans le cas d'un signal invalide, \textit{x} vaut "*" et \textit{rtt} et \textit{from} sont absents. Nous précisons qu'un signal est invalide quand la sonde ne reçoit pas une réponse après le temps \textit{tiemout}. Afin d'adapter un saut aussi dans le cas de l'absence des détails du signal, les attributs \textit{rtt}, \textit{x}  et \textit{from} sont définis comme étant optionnels.  
Un signal est modélisé par la classe Signal (voir le Listing \ref{lst:case-class-signal}). 

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la  classe Signal}, label={lst:case-class-signal}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class Signal(
	rtt:  Option[Double],
	x:    Option[String],
	from: Option[String])
\end{lstlisting}

Nous avons défini  la classe \textit{Traceroute} qui nous permet de lire les données. Pour ce faire, nous utilisons l'objet \textit{spark} de type \textit{SparkSession} créé précédemment dans le Listing \ref{lst:SparkSession-initiation}. En particulier, nous faisons appel à   la fonction \textit{read()} via \textit{spark}. Nous spécifions à \textit{read()} le schéma de lecture à travers la classe Traceroute, le chemin vers les fichiers de données (\textit{dataPath}) et comment  les données  sont structurées (\textit{json}). 

\begin{lstlisting}[language=scala,firstnumber=1, caption={La lecture des données traceroutes},label={lst:mapping}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val rawTraceroutes = spark.read
	.schema(Encoders.product[Traceroute].schema)
	.json(dataPath)
	.as[Traceroute]
import spark.implicits._ |\label{lst:implicits}|
 \end{lstlisting}

Nous obtenons la liste des traceroutes dans la variable \textit{rawTraceroutes}. Ce dernier est un Dataset d'objets  Traceroute.
La correspondance entre un enregistrement traceroute JSON et une instance de Traceroute se base sur les noms des attributs dans JSON.
Nous notons que la réussite de la correspondance ne nécessite pas l'association de tous les attributs de l'objet JSON.   Si par exemple, nous définissons des attributs dans la classe Traceroute et que ces attributs ne font pas partie des attributs de l'objet JSON, aucune erreur ne sera survenue.  
La ligne \ref{lst:implicits} du Listing \ref{lst:mapping}  est nécessaire  pour pouvoir utiliser  l'API du Spark, précisément,  les fonctionnalités relatives aux DataSets et aux DataFrames. L'appel à la ligne \ref{lst:implicits} du Listing \ref{lst:mapping} ne peut pas être fait sans avoir une instance du \textit{SparkContext}. Dans notre cas, nous avons défini une instance de \textit{SparkSession}, et  l'instance  du \textit{SparkContext} est fournie, par défaut,  avec \textit{SparkSession}.


% A , nous appelons certaines fonctionnalités nécessaires à la lecture des données. Il est important de noter que Apache Spark adopte ce qu'on appelle \textit{lazy evaluation}; %(voir la section \ref{lazy-evaluation}). Ainsi,
 %l'évaluation des différentes transformations ne s'effectuent qu'au moment du déclenchement d'une action sur le résultat de cette transformation.

\paragraph{Trouver les périodes de l'analyse}~

Dès à présent, la liste des traceroutes est prête à toute transformation de la phase I (voir la phase I dans la section \ref{processus-de-detection}). 
Tout d'abord, nous devons trouver les périodes entre la date de début et la date de fin (étape \textit{FindBins} (I.1)). 
Cette étape est illustrée par la fonction \textit{generateDateSample}  dans le Listing \ref{lst:findbins};  
 nous construisons les tuples de périodes afin de faciliter le test d'appartenance d'un traceroute, un tuple est formé par le début de la période (\textit{start}),  \textit{start+timewindow}, nous prenons \textit{timewindow} comme étant  équivalent à une heure.


\begin{lstlisting}[language=scala,firstnumber=1, caption={Etape FindBins (I.1)},label={lst:findbins}, basicstyle = \small,escapechar=|,numbers=left,
stepnumber=1]
//Generate the start of all  bins : between start date and end date espaced by the timewindow
val rangeDates = generateDateSample(start, end, timewindow)

// Find the start and the end of each bin
val rangeDatesTimewindows = rangeDates.map(f => (f, f + timewindow))
\end{lstlisting}

A la fin de cette étape, toutes les périodes sont déterminées. Nous passons à l'étape du groupement des traceroutes, disponibles à l'analyse, par période (étape I.2).

\paragraph{Groupement des traceroutes}~

L'objectif de cette étape est de grouper les traceroutes capturés par période. 
Dans l'implémentation du travail de référence \cite{DBLP:journals/corr/FontugneAPB16}, les données sont organisées dans des collections MongoDB (voir la section \ref{subsubsection:mongodb}), le groupement des traceroutes par période se base sur la structuration des noms des collections\footnote{Voir la section \ref{mongodb-impleme}.}. Pour une période donnée, seules les collections concernées  seront interrogées.  Nous présentons le groupement selon MongoDB et selon Spark/Scala :
 

\subparagraph{MongoDB}
Nous résumons dans la Figure \ref{fig:read-data-from-mongodb} le groupement  tel qu'il est présenté dans le travail de référence.  
Selon la période en question, nous interrogeons la collection  adéquate.

%A chaque période, les traceroutes sont sélectionnés de la base de données MongoDB en se basant sur les noms des collections. 
\begin{figure}[h]
	\centering
	
	\captionsetup{justification=centering}
	\resizebox{10cm}{6cm}{
	\input{illustrations/read-data-from-mongodb.tex}
}
	\caption{Groupement des traceroutes avec MongoDB}
	\label{fig:read-data-from-mongodb}
\end{figure}

Nous illustrons ce groupement avec un pseudo-code décrit par l'algorithme \ref{group-traceroutesmongodb}.

\begin{algorithm}[H]
	\caption{Groupement des traceroutes dans le cas de MongoDB}
	\label{group-traceroutesmongodb}
	\begin{algorithmic}
		%\For{$traceroute$ $\in$ $rawTraceroutes$} 
		%\State \texttt{Attribuer $traceroute$ à }
		
		\For{$period$ $\in$ $rangeDatesTimewindows$} 
		\State $collection$ $\gets$ \texttt{Trouver la collection incluant $period$}
		\State \texttt{$rawTraceroutes$ $\gets$   les traceroutes stockés dans  $collection$ et enregistrés durant $period$}
		
     ...	\Comment{Traitements appliqués sur l'ensemble de traceroutes}
		%\State \texttt{Vérifier si  $traceroute$ appartient à  $period$}
		\EndFor
		%\EndFor
	\end{algorithmic}
\end{algorithm}

Cette manière de grouper les traceroutes ne prend pas en considération le cas où
il faut chercher les traceroutes dans plus d'une collection.

\subparagraph{Spark/Scala} En ce qui concerne l'implémentation en Spark/Scala,
 nous avons groupé les traceroutes autrement. 
 %en partant de ces derniers. 
 Les traceroutes sont organisés dans des fichiers qui peuvent contenir des traceroutes d'une heure, d'une journée ou de toute autre période.  Peu importe le nombre de ces fichiers, Spark lit tout ce qui est disponible dans le chemin \textit{dataPath}. 
 Notons que le  parcours de tous les fichiers à chaque  période est coûteux en terme de performance.
  C'est pourquoi nous avons attribué les traceroutes aux périodes. Dans ce cas, les fichiers de données sont lus une seule fois. C'est ce que nous résumons dans l'algorithme \ref{group-data-sparkscala}. Les lignes entre \ref{initate-list} et \ref{finloop1} de l'algorithme \ref{group-data-sparkscala} permettent d'associer chaque traceroute à une période faisant partie de la période de l'analyse. La ligne  \ref{groupby} permet de grouper les traceroutes par période; il s'agit de grouper les traceroutes ayant la même période. L'objectif des lignes entre \ref{begin-groupby} et \ref{end-groupby}  de l'algorithme \ref{group-data-sparkscala} est d'appliquer les traitements sur les groupes de traceroutes, cependant, elles ne sont pas détaillés dans cet algorithme. 
  
  %Seuls les traceroutes qui font partie de la période de l'analyse à un traceroute, on teste toutes les périodes jusqu'à trouver la période adéquate.

\begin{algorithm}[H]
	\caption{Groupement des traceroutes en Spark}
	\label{group-data-sparkscala}
	\begin{algorithmic}[1]
    \State $ traceroutePerPeriod $  $\gets$ [] \label{initate-list}
	\For{$traceroute$ $\in$ $rawTraceroutes$} 
	%\State \texttt{Attribuer $traceroute$ à }
	
		\For{$period$ $\in$ $rangeDatesTimewindows$} 
			\State \texttt{Vérifier si  $traceroute$ appartient à  $period$}
		\EndFor
		\State \texttt{$ traceroutePerPeriod $.append(($ traceroute $, $period$))}
	\EndFor  \label{finloop1}
	\State \texttt{$ traceroutesPerPeriods $ $\gets$$ traceroutePerPeriods $.groupBy($period$)} \label{groupby}
	\For{$element$ $\in$ $ traceroutesPerPeriods $}\label{begin-groupby}
	\State ... \Comment{Traitements appliqués sur l'ensemble de traceroutes}
	\EndFor \label{end-groupby}

\end{algorithmic}
\end{algorithm}

En Spark/Scala, nous avons chargé les traceroutes disponibles à l'analyse sur un RDD. Ce dernier crée des partitions de données, sensées être manipulées sur différentes machines si Spark est lancé sur un \textit{cluster} de machine, autrement dit, en mode \textit{cluster}. Afin de créer les groupes de traceroutes, nous vérifions l'appartenance de chaque traceroute à une des périodes considérées.

\begin{lstlisting}[language=scala,firstnumber=1, caption={},label={lst:groupalltraceroutes}, basicstyle = \small,escapechar=|,numbers=left,
stepnumber=1]
//Group each traceroute by the bin that they belong in 
// If one traceroute does not belongs in any bin, then by default it belongs to the bin 0
val tracerouteAndPeriodRdd = rawTraceroutes.rdd.map(traceroute => TracerouteWithTimewindow(traceroute, findTimeWindowOfTraceroute(traceroute, rangeDatesTimewindows)))|\label{line:groupTraceroutes}|
\end{lstlisting}

Avec la ligne \ref{line:groupTraceroutes} dans le Listing \ref{lst:groupalltraceroutes} :

\begin{itemize}
	\item nous transformons  \textit{rawTraceroutes}  en un RDD;
	\item pour chaque objet Traceroute, nous appliquons le traitement du groupement en utilisant la méthode \textit{findTimeWindowOfTraceroute}. Cette dernière prend en paramètre le traceroute et les périodes possibles dont ce dernier peut y appartient et elle renvoie la période adéquate;
	\item nous passons d'un RDD de type \textit{Traceroute} à un RDD de type \textit{TracerouteWithPeriod}.
\end{itemize}


La  classe \textit{TracerouteWithPeriod}, définie dans le Listing \ref{lst:TracerouteWithTimewindow}, permet de représenter un traceroute avec sa période dans une seule entité. 
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe TracerouteWithPeriod },label={lst:TracerouteWithTimewindow}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class TracerouteWithPeriod(
	traceroute: Traceroute,
	period:     Int)
\end{lstlisting}

\paragraph{Elimination des traceroutes qui n'appartiennent pas à l'analyse}~

Il se peut qu'un traceroute ne fait pas partie de la période de l'analyse. Ce sont les objets de type \textit{TracerouteWithPeriod}  où  \textit{period} vaut $0$ par construction. Ce sont les traceroutes ayant un   \textit{timestamp}  qui n'appartient à aucune des période. C'est pourquoi nous filtrons ces traceroutes, comme  illustre le Listing  \ref{lst:filterTracerouteWithTimewindow}.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Elimination des traceroutes non concernés par l'analyse },label={lst:filterTracerouteWithTimewindow}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val onlyConcernedTraceroutes = tracerouteAndPeriodRdd.filter(_.period != 0)
\end{lstlisting}

Après l'élimination des traceroutes non concernés, nous agrégeons ces derniers par période pour construire une liste de type   \textit{TraceroutesPerPeriod} dont sa définition est  donnée dans le Listing \ref{lst:classTraceroutesPerPeriod}. 
L'objectif de cette agrégation est de créer des groupes de traceroutes et y  appliquer les traitements relatifs à la détection. 
Le Listing \ref{lst:agregatePeriodTraceroutes} illustre l'étape de l'agrégation, la ligne 
\ref{line:groupeby} effectue le groupement et la ligne \ref{line:resumegroupeby} organise les groupes de traceroutes dans une nouvelle structure de données : \textit{TraceroutesPerPeriod}.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Le groupement des traceroutes par période},label={lst:agregatePeriodTraceroutes}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val groupedTraceroutesByPeriod = onlyConcernedTraceroutes.groupBy(_.period)|\label{line:groupeby}|
val traceroutesPerPeriod = groupedTraceroutesByPeriod.map(f => TraceroutesPerPeriod(f._2.map(twp => twp.traceroute).toSeq, f._1)) |\label{line:resumegroupeby}|
\end{lstlisting}


\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe TraceroutesPerPeriod },label={lst:classTraceroutesPerPeriod}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class TraceroutesPerPeriod(
	traceroutes: Seq[Traceroute],
	period:  Int)
\end{lstlisting}

\paragraph{Déduction des liens}~

L'étape qui suit le groupement des traceroutes est la génération des liens. Ainsi, nous générons les différents liens possibles dans chacune des périodes avec le code du Listing \ref{lst:linkInference} :

\begin{lstlisting}[language=scala,firstnumber=1, caption={Etape de déduction des liens},label={lst:linkInference}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val allLinksRttDiffsPeriods = traceroutesPerPeriod.map(f => linksInference(spark, f))
\end{lstlisting}

La fonction \textit{linksInference} est une abstraction de plusieurs traitements appliqués sur chaque groupe de traceroutes.  Cette fonction renvoie
la liste des liens caractérisés par leurs périodes et RTTs différentiels. Les étapes aboutissant à ces résultats  sont : 

\begin{enumerate}
	\item élimination des traceroutes échoués;
	\item élimination des sauts non valides;
	\item calcul de la médiane de chaque saut;
	\item énumération des liens possibles par traceroute en encapsulant ces derniers dans des objets \textit{DetailedLink};
	\item construction d'une liste reprenant les listes de l'étape $4$.
	\item tri alphanumérique des adresses IP de chaque lien;
	\item groupement des liens ayant les mêmes adresses IP;
	\item création d'un récapitulatif de chaque lien de toutes les périodes; chaque lien est associé à une liste des RTTs différentiels ainsi qu'à une liste des périodes. Cette dernière contient la période associée à un RTT différentiel dupliquée en nombre des RTTs différentiels de ce lien durant cette période. 
\end{enumerate}

%Les étapes énumérées ci-dessus sont illustrées par le Listing \ref{lst:DeductionLinks}.
Le Listing \ref{lst:DeductionLinks}
illustre les étapes de la déduction des liens par groupe de traceroutes.
\begin{lstlisting}[language=scala,firstnumber=1, caption={Deduction des liens par groupe de traceroutes},label={lst:DeductionLinks}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
 def linksInference(spark: SparkSession, rawtraceroutes: TraceroutesPerPeriod): Seq[SummarizedLink] = {
	
	//Filter failed traceroutes ... 
	val notFailedTraceroutes = rawtraceroutes.traceroutes.filter(t => t.result(0).result != null) |\label{notFailedTraceroutes}|
	
	//Remove invalid data  in hops
	val cleanedTraceroutes = notFailedTraceroutes.map(removeInvalidSignals) |\label{cleanedTraceroutes}|
	
	//Compute median by hop
	val tracerouteMedianByHop = cleanedTraceroutes.map(computeMedianRTTByhop)|\label{tracerouteMedianByHopline}|
	
	//Find links in a traceroute
	import org.apache.spark.mllib.rdd.RDDFunctions._
	val tracerouteLinks = tracerouteMedianByHop.map(t => findLinksAndRttDiffByTraceroute(spark, t)) |\label{tracerouteLinksline}|
	
	//Create a set of DetailedLink objects for every traceroute
	val detailedLinks = tracerouteLinks.map(summarizeLinksTraceroute) |\label{liens-par-périodeline}|
	
	//Flatten the list of lists to have one liste of DetailedLink objects
	val allDetailedLinks = detailedLinks.flatten |\label{allDetailedLinksFlattenline}|
	
	//Sort the links
	val sortAllDetailedLinks = allDetailedLinks.map(sortLinks) |\label{sortAllDetailedLinksline}|
	
	//Merge the links from all traceroutes in the current bin
	val mergedLinks = sortAllDetailedLinks.groupBy(_.link) |\label{mergedLinksline}|
	
	//Summarize the link 
	val summarizeLink = mergedLinks.map(f => SummarizedLink(f._1, f._2.map(_.probe), f._2.map(_.rttDiff), generateDatesSample(f._2.size, rawtraceroutes.timeWindow)))  |\label{summarizeLinkline}|
	
	summarizeLink.toSeq    
}
\end{lstlisting}


Nous reprenons dans les sections suivantes la définition des fonctions utilisées dans la fonction \textit{linksInference}.

\subparagraph{Elimination des traceroutes échoués} (Ligne \ref{notFailedTraceroutes} dans le Listing \ref{lst:DeductionLinks}) Nous parcourons chaque traceroute de la liste des traceroutes \textit{rawtraceroutes.traceroutes} afin d'éliminer tout traceroute échoué. Dans l'implémentation de l'outil de détection, ils ont constaté que la liste des sauts d'un traceroute échoué contient un seul saut (\textit{t.result(0)}) et sa liste de signaux n'existe pas (\textit{t.result(0).result != null}). Avec \textit{t} dénote le traceroute courant dans la liste \textit{rawtraceroutes.traceroutes}. 


\subparagraph{Elimination des sauts non valides} (Ligne \ref{cleanedTraceroutes} dans le Listing \ref{lst:DeductionLinks}) Nous parcourons les traceroutes valides (\textit{notFailedTraceroutes}) et nous appliquons  \textit{removeInvalidSignals} sur chaque traceroute. Cette fonction renvoie un traceroute après avoir appliqué la fonction \textit{checkSignal} sur chaque signal de chaque saut. 

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la méthode removeInvalidSignals},label={lst:removeInvalidSignals}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
  def removeInvalidSignals(traceroute: Traceroute): Traceroute = {
	val hops = traceroute.result
	for (hop <- hops) {
		val signals = hop.result
		val tmpSignals = signals.filter(checkSignal(_))
		hop.result = tmpSignals
	}
	traceroute
}
\end{lstlisting}

Le rôle de la fonction \textit{checkSignal}, définie dans le Listing \ref{lst:checkSignal},  est de vérifier chaque signal : si le signal est valide (Ligne \ref{timesout}), si le RTT existe (Ligne \ref{rttnotnone}), si le RTT est positif  (Ligne \ref{rttpositif}) et enfin si le signal ne provient pas d'une adresse IP privée (Ligne \ref{notprivate}). Nous avons utilisé les expressions régulières pour vérifier si l'adresse IP est publique ou bien privée. Nous avons profité de l'interopérabilité du Scala avec Java et nous avons  utilisé la classe java.util.regex.Pattern.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la méthode checkSignal},label={lst:checkSignal}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
  def checkSignal(signal: Signal): Boolean = {
	//Check if a signal is not failled
	if (signal.x == "*") |\label{timesout}|
	return false
	// Check if the RTT exist
	else if (signal.rtt == None) |\label{rttnotnone}|
	return false
	else if (signal.rtt.get <= 0) { |\label{rttpositif}|
		return false
	} else if (javatools.Tools.isPrivateIp(signal.from.get)) |\label{notprivate}|
	return false
	else {
		return true
	}
  }
\end{lstlisting}

\subparagraph{Calcul de la médiane de chaque saut} (Ligne \ref{tracerouteMedianByHopline} dans le Listing \ref{lst:DeductionLinks})
Après avoir vérifié les sauts des traceroutes, nous appelons la fonction \textit{computeMedianRTTByhop}  présentée dans le Listing \ref{lst:computeMedianRTTByhopFunction} pour calculer la médiane des RTTs par saut. Dans un premier temps, la fonction \textit{computeMedianRTTByhop} récupère les sauts du traceroute donné en entrée, elle calcule la médiane des RTTs et renvoi un traceroute avec des RTTs agrégés. 

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la fonction computeMedianRTTByhop},label={lst:computeMedianRTTByhopFunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]  
  def computeMedianRTTByhop(traceroute: Traceroute): MedianByHopTraceroute = {
	val hops = traceroute.result
	val procHops = hops.map(f => findMedianFromSignals(f))
	s
	MedianByHopTraceroute(traceroute.dst_name, traceroute.from, traceroute.prb_id, traceroute.msm_id, traceroute.timestamp, procHops)
}
\end{lstlisting}

Le traceroute renvoyé est de type \textit{MedianByHopTraceroute}. Ce dernier est défini dans le Listing \ref{lst:MedianByHopTracerouteclass}. Une instance de la classe \textit{Traceroute} est  diffente  d'une instance de la classe \textit{MedianByHopTraceroute} au niveau de l'attribut \textit{result} qui désigne la liste des sauts.
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe MedianByHopTraceroute},label={lst:MedianByHopTracerouteclass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class MedianByHopTraceroute(
	dst_name:  String,
	from:      String,
	prb_id:    BigInt,
	msm_id:    BigInt,
	timestamp: BigInt           = 0,
	result:    Seq[PreparedHop])
\end{lstlisting}

La nouvelle classe (\textit{PreparedHop}) qui représente un saut est définie dans le Listing \ref{lst:PreparedHopclass}.
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe PreparedHop},label={lst:PreparedHopclass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class PreparedHop(
	var result: Seq[PreparedSignal],
	hop:        Int)
\end{lstlisting}

Dans la nouvelle définition du saut, ce dernier est toujours défini par une liste de signaux, car la sonde peut recevoir les signaux, pour un même saut, depuis différents routeurs. La nouvelle classe représentant un signal est définie dans le Listing \ref{lst:PreparedSignalclass}, où \textit{medianRtt} est la médiane des RTTs en provenance du routeur \textit{from}. 
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe PreparedSignal},label={lst:PreparedSignalclass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class PreparedSignal(
	medianRtt: Double,
	from:      String)
\end{lstlisting}

\subparagraph{Enumération des liens possibles par traceroute et leur RTT différentiel} (Ligne \ref{tracerouteLinksline} du Listing \ref{lst:DeductionLinks})  La fonction \textit{findLinksAndRttDiffByTraceroute} prend en paramètre une instance de \textit{MedianByHopTraceroute} et renvoie une instance de \textit{LinksTraceroute}. Les liens sont déduits suivant l'approche décrite dans la Figure \ref{fig:link-inference_}. Le rôle de cette méthode est de parcourir les sauts afin de construire les liens possibles. Précisément, c'est la fonction findAllLinks (Ligne \ref{findAllLinksline} du Listing \ref{lst:functionfindLinksAndRttDiffByTraceroute}) qui crée les liens possibles pour tous deux sauts consécutifs et calcule leur RTT différentiel.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la fonction findLinksAndRttDiffByTraceroute},label={lst:functionfindLinksAndRttDiffByTraceroute}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
  def findLinksAndRttDiffByTraceroute(spark: SparkSession, traceroute: MedianByHopTraceroute): LinksTraceroute = {
	val hops = traceroute.result
	val size = hops.size
	val s = hops.zipWithIndex
	val z = s.map {
		case (element, index) =>
		if (index + 1 < size) {
			findAllLinks(hops(index + 1), element) |\label{findAllLinksline}|
		} else {
			null
		}
	}
	return new LinksTraceroute(traceroute.dst_name, traceroute.from, traceroute.prb_id, traceroute.msm_id, traceroute.timestamp, z.filter(p => p != null).flatten)
}
\end{lstlisting}

 La fonction \textit{findAllLinks}, définie dans le Listing \ref{lst:findAllLinksfunction}, prend deux paramètres de type \textit{PreparedHop} : \textit{nextHop} et \textit{currentHop}. Cette fonction renvoie la liste des liens (\textit{links}) avec leur RTT différentiel. 
 
 \begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la fonction findAllLinks},label={lst:findAllLinksfunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
 stepnumber=1] 
  def findAllLinks(nextHop: PreparedHop, currentHop: PreparedHop): Seq[Link] = {
	var links = Seq[Link]()
	
	for (nextRouter <- nextHop.result) {
		for (currentRouter <- currentHop.result) {
			val rttDiff = BigDecimal(nextRouter.medianRtt) - BigDecimal(currentRouter.medianRtt)
			links = links :+ Link(nextRouter.from, currentRouter.from, rttDiff.toDouble)
		}
	}
	return links
}
\end{lstlisting}

Après avoir parcouru les sauts consécutifs du traceroute, nous créons une instance de la classe \textit{LinksTraceroute}. Cette classe reprend la liste des liens au lieu de la liste des sauts avec les autres informations du traceroute. La définition de cette classe est donnée dans le Listing \ref{lst:classLinksTraceroute}.
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe LinksTraceroute},label={lst:classLinksTraceroute}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class LinksTraceroute(
	dst_name:  String,
	from:      String,
	prb_id:    BigInt,
	msm_id:    BigInt,
	timestamp: BigInt,
	links:     Seq[Link])
\end{lstlisting}

La classe \textit{Link} modélise un lien et son RTT différentiel (voir le Listing \ref{lst:Linkclass}).
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe Link},label={lst:Linkclass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class Link(
	ip1:     String,
	ip2:     String,
	rttDiff: Double)
\end{lstlisting}

\subparagraph{Construction de la liste des liens par période } (Ligne \ref{liens-par-périodeline} du Listing \ref{lst:DeductionLinks})
%	\item lister les liens possibles par traceroute en encapsulant ces derniers dans des objets \textit{DetailedLink};tracerouteLinks
A cette étape, nous souhaitons réorganiser les liens précédemment énumérés par traceroute. L'objectif de cette réorganisation est de passer des liens identifiés dans le cadre d'un traceroute (une instance de \textit{LinksTraceroute}) à une liste de liens (plusieurs instances de \textit{DetailedLink}) comme illustre la définition de la fonction summarizeLinksTraceroute dans le Listing \ref{lst:summarizeLinksTraceroutefunction}. 


\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la fonction summarizeLinksTraceroute},label={lst:summarizeLinksTraceroutefunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
  def summarizeLinksTraceroute(traceroute: LinksTraceroute): Seq[DetailedLink] = {
	val links = traceroute.links
	val summarizedLinks = links.map(f => DetailedLink(f.rttDiff, LinkIPs(f.ip1, f.ip2), traceroute.prb_id))
	summarizedLinks
}
\end{lstlisting}

La classe \textit{DetailedLink}, définie dans le Listing \ref{lst:DetailedLinkclass},   modélise un seul lien.  Le mot clé \textbf{\textit{var}} rend l'attribut \textit{link} mutable.
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe DetailedLink},label={lst:DetailedLinkclass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
case class DetailedLink(
	rttDiff:  Double,
	var link: LinkIPs,
	probe:    BigInt)
\end{lstlisting}

La fonction \textit{linksInference} est appliquée sur chaque groupe de traceroutes. Dans ce cas, pour chaque groupe de traceroutes, on associe une liste des liens encapsulés chacun dans une instance de \textit{DetailedLink}. Nous avons utilisé la fonction \textit{flatten} pour regrouper tous les liens dans un groupe de traceroute, c'est ce que fait la ligne \ref{allDetailedLinksFlattenline} dans la fonction dans le Listing \ref{lst:DeductionLinks}.
%[!]Maintenant que chaque lien reprend l'information de la période pendant laquelle il a été identifié, nous pouvons créer une liste d'instance de \textit{DetailedLink} concernant toutes les périodes. 

\subparagraph{Tri alphanumérique des adresses IP de chaque lien} (Ligne \ref{sortAllDetailedLinksline} dans le Listing \ref{lst:DeductionLinks}). La méthode \textit{sortLinks}, définie dans le Listing \ref{lst:sortLinksfunction}, permet de trier les deux adresses IP du lien donné en paramètre et renvoi un nouveau lien. Cette fonction s'applique sur tout lien (\textit{DetailedLink}).
\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la fonction sortLinks},label={lst:sortLinksfunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1] 
def sortLinks(linkToSort: DetailedLink): DetailedLink = {
	val link = Seq(linkToSort.link.ip1, linkToSort.link.ip2)
	val sortedLink = link.sorted
	linkToSort.link = LinkIPs(sortedLink(0), sortedLink(1))
	linkToSort
}
\end{lstlisting}


\subparagraph{Groupement des liens ayant les mêmes adresses IP} (Ligne \ref{mergedLinksline} dans le Listing \ref{lst:DeductionLinks})
Après avoir ordonné les deux adresses IP des liens, nous regroupons les liens par leurs adresses IP en utilisant la fonction GroupBy de Scala.

\subparagraph{Création d'un récapitulatif de chaque lien} (Ligne \ref{summarizeLinkline} dans le Listing \ref{lst:DeductionLinks})
C'est la dernière étape de la phase de la préparation de données.  L'objectif de cette étape est de regrouper les même lien dans une seule entité, représentée par une instance de la classe \textit{SummarizedLink} définie dans le Listing \ref{lst:SummarizedLinkClasscase}.
 Cette classe reprend les deux  adresses IP du lien (\textit{link}), la liste des sondes ayant identifié ce lien (\textit{probes}), la liste des RTTs différentiels de ce lien (\textit{rttDiffs}) et enfin  \textit{periods} qui  représentent les périodes  pendant lesquelles les \textit{rttDiffs} ont été identifiés. 


\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe SummarizedLink},label={lst:SummarizedLinkClasscase}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
case class SummarizedLink(
	link:     LinkIPs,
	probes:   Seq[BigInt],
	rttDiffs: Seq[Double],
	var periods: Seq[Int])
\end{lstlisting}


\paragraph{Caractérisation des liens de toutes les périodes de l'analyse}~

Après avoir traité tous les groupes de traceroutes, nous obtenons un RDD de liste de liens (\textit{RDD[Seq[classes.SummarizedLink]]}). Nous devons collecter les résultats des traitements de chacune des partitions de ce RDD afin de passer à la phase II de l'analyse des délais. Dans le cas d'un \textit{cluster} de machines, il s'agit de la collecte de ces résultats de la part de chaque machine.  
Le code illustrant la collecte des résultats est illustrée par le Listing \ref{lst:rddcollecte}. Autrement dit, le \textit{driver}  reçoit les résultats des traitements appliqués sur les données et effectués par les \textit{workers}\footnote{Voir le principe de la distribution des traitements en Spark dans la section \ref{sparkpresentationsection}.}. Nous appliquons ensuite la fonction \textit{flatten} sur les collectés.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Collecte des résultats intermédiares },label={lst:rddcollecte}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
val finalLinksDetailsList = allLinksRttDiffsPeriods.collect().toSeq.flatten
\end{lstlisting}

Après avoir collecté les résultats intermédiaires, nous fusionnons les données des liens en provenance de toutes les périodes. C'est ce que illustre le Listing \ref{lst:rddmergeLinks}. La différence entre cette collecte et la collecte précédente (Ligne \ref{allDetailedLinksFlattenline} dans le Listing \ref{lst:DeductionLinks}), c'est que la première a été faite au niveau d'un \textit{worker}, or, la collecte précédente est faite au niveau du \textit{driver}.

\begin{lstlisting}[language=scala,firstnumber=1, caption={Fusion des liens de toute la période de l'analyse},label={lst:rddmergeLinks}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]

// Merge all links from all periods
val finalResult = collectedRTTDiff.groupBy(_.link)
val finalRawRttDiff = finalResult.map(f => SummarizedLink(
                                            f._1, 
                                            (f._2.map(_.probes)).flatten, 
                                            (f._2.map(_.rttDiffs)).flatten, 
                                            (f._2.map(_.periods)).flatten)
                                     )
\end{lstlisting}

Commentaires  du Listing \ref{lst:rddmergeLinks} :

\noindent \textit{finalResult} est de type Map  : (\textit{Map[classes.LinkIPs, Seq[classes.SummarizedLink]]}), la clé est de type \textit{LinkIPs} et la valeur est de type
 \textit{Seq[classes.SummarizedLink]}.

\noindent En parcourons  \textit{finalResult}, l'élément courant parcouru $f$ contient deux parties, vu qu'il appartient à une Map, la première, $ f\_1 $, représente la clé (de type \textit{LinkIPs})  et la deuxième,  $ f\_2 $, représente la valeur (de type  \textit{Seq[classes.SummarizedLink]}).  Ainsi, le rôle de \textit{f.\_2.map(\_.periods)). flatten} est de parcourir, via la fonction \textit{map}, la liste des liens, et de récupérer en particulier la liste des périodes stockée dans \textit{periods} pour ensuite regrouper toutes ces période en une seule liste (via \textit{flatten}).


A cette étape, nous avons une liste de type \textit{SummarizedLink} qui concerne  la période entière de l'analyse. 
%Ce dernier est définie dans le Listing \ref{lst:SummarizedLinkClasscase}. 

%\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la classe ResumedLink},label={lst:ResumedLinkClass}, basicstyle = \footnotesize,escapechar=|,numbers=left,
%stepnumber=1]
%case class ResumedLink(
%	link:     LinkIPs,
%	probes:   Seq[BigInt],
%	rttDiffs: Seq[Double],
%	var bins: Seq[Int])
%\end{lstlisting}


\paragraph{Détection des anomalies}~

Nous présentons dans ce qui suit la phase II de l'analyse des délais. A travers la méthode \textit{listAlarms()} nous analysons un lien et nous identifions les anomalies de ce dernier. Dans le Listing \ref{lst:paralelizeAndDtectAnomalies}, d'abord nous convertissons la liste des liens en un RDD afin de distribuer le traitement de ces liens. Ensuite, nous appliquons la méthode \textit{listAlarms} sur tout lien. 


A la fin de la phase I, nous obtenons une liste des liens de type \textit{Iterable}  

(Iterable[classes.SummarizedLink]). Comme chaque lien peut être traité indépendamment des autres liens, nous créons un RDD à partir de cette liste (Ligne \ref{parallelizefinalRawRttDiff} du Listing \ref{lst:paralelizeAndDtectAnomalies}), ensuite, nous appliquons la méthode sur chaque lien (Ligne \ref{listAlarmscall} du Listing \ref{lst:paralelizeAndDtectAnomalies}).  

\begin{lstlisting}[language=scala,firstnumber=1, caption={Détection des alarmes des liens},label={lst:paralelizeAndDtectAnomalies}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
// Create a RDD having the SummarizedLink elements from stage I
val finalRawRttDiffRdd = spark.sparkContext.parallelize(finalRawRttDiff.toSeq) |\label{parallelizefinalRawRttDiff}|

// Alarms detection by link
val linkAnalysisResult = finalRawRttDiffRdd.map(p => listAlarms(spark, p, timewindow, rangeDates)) |\label{listAlarmscall}|
\end{lstlisting}

La méthode \textit{listAlarms} est détaillée dans le Listing \ref{lst:findAnalomalies}. Dans cette dernière, nous assurons:


\begin{enumerate}
	\item Initialisation des variables : \textit{reference} est l'état référence du lien, \textit{current} est l'état courant du lien, \textit{alarmsValues} est la liste des alarmes qui sont des RTTs différentiels médians, \textit{alarmsDates} est la liste des dates correspondantes aux alarmes, \textit{dates} est la liste des dates concernées; ce sont les périodes ayant une distribution des RTTs différentiels de taille plus grande d'un nombre donné.
	\item  Génération des périodes; nous générons les périodes correspondantes à au moins une journée. Par construction, les périodes générées à la phase I sont les mêmes générées à la phase II.
    \item  En partant des périodes générées, dans leurs ordre chronologique, nous appliquons la méthode \textit{findAlarms()} sur les RTTs différentiels de chaque période. 
    \item Enfin, nous construisons un objet JSON reprenant les détails du lien. A savoir, leurs périodes, leurs anomalies et leurs dates d'anomalies. 
\end{enumerate}

\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la méthode listAlarms},label={lst:findAnalomalies}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
  def listAlarms(spark: SparkSession, rawDataLinkFiltred: SummarizedLink, timewindow: Int, rangeDates: Seq[Int]): String = {
	// Save the reference state of a link
	var reference = LinkState(Seq(), Seq(), Seq(), Seq())
	
	// Save the current state of a link
	var current = LinkState(Seq(), Seq(), Seq(), Seq())
	
	// Save the RTT differentials anomalies
	var alarmsValues = AlarmsValues()
	
	// Save the dates having delay anomalies
	var alarmsDates = AlarmsDates()
	
	// Save all the dates to draw the evolution 
	var dates = AllDates()
	
	val rawDataLink = rawDataLinkFiltred
	
	/*Regardless of the period specified in the inputs, the evolution is created for one or more days
	* Eg : if the period is only 2 hours, the evolution is created for 24 hours,
	* and the begin date is the begin date given in inputs
	* */
	val start = rawDataLink.bins.min
	val max = rawDataLink.bins.max
	val diferenceDays = (max - start) / 60 / 60 / 24
	val end = start + ((diferenceDays + 1) * 86400)
	
	//Find all the bins in the selected days
	val datesEvolution = start.to(end - timewindow).by(timewindow)
	
	// For each bin, find the data (RTTs differentials) and find alarms
	datesEvolution.foreach(f => findAlarms(spark, f, reference, rawDataLink, current, alarmsDates, alarmsValues, dates))
	
	// create a JSON string to save the results
	implicit val formats = DefaultFormats
	val linkEvolution = LinkEvolution(rawDataLink.link, reference, current, alarmsDates.dates, alarmsValues.medians, dates.dates)
	val linkEvolutionJsonStr = write(linkEvolution)
	linkEvolutionJsonStr
}
\end{lstlisting}

La définition de la fonction \textit{findAlarms} est donnée dans le Listing \ref{lst:findAlarmsFunction}. Cette méthode s'applique sur les données d'un seul lien,
son objectif  est de comparer l'état courant du lien en question avec la référence suivant les trois cas détaillés dans l'étape II.5 du processus décrit dans la section \ref{processus-de-detection}.


\begin{lstlisting}[language=scala,firstnumber=1, caption={Définition de la méthode findAlarms},label={lst:findAlarmsFunction}, basicstyle = \footnotesize,escapechar=|,numbers=left,
stepnumber=1]
  def findAlarms(spark: SparkSession, date: Int, reference: LinkState, dataPeriod: SummarizedLink, current: LinkState, alarmsDates: AlarmsDates, alarmsValues: AlarmsValues, dates: AllDates): Unit = {
	println("Find indices ...")
	val indices = dataPeriod.bins.zipWithIndex.filter(_._1 == date).map(_._2) |\label{lst:lineindices}|
	val dist = indices.map(f => dataPeriod.rttDiffs(f)) |\label{lst:linedist}|
	
	println("Find RTTs for the current timewindow ...")
	val distSize = dist.size
	
	if (distSize > 3) {  |\label{lst:linedistSizesup}|
		val tmpDates = dates.dates :+ date
		dates.dates = tmpDates
		
		// Compute the Wilson Score
		val wilsonCi = scoreWilsonScoreCalculator(spark, dist.size).map(f => f * dist.size) |\label{lsl:linewilson}|
		
		//update the current link state
		updateLinkCurrentState(spark, dist, current, wilsonCi) |\label{lst:lineupdateLinkCurrentState}|
		
		//Sort the distribution
		val newDist = dist.sorted |\label{linenewDist}|
		
		//Get the reference
		val tmpReference = reference |\label{line-getreference}|
		
		// Case : 1
		if (tmpReference.valueMedian.size < 3) { |\label{lsl-debutcases}|
			val newReferenceValueMedian = tmpReference.valueMedian :+ current.valueMedian.last
			val newReferenceValueHi = tmpReference.valueHi :+ newDist(javatools.JavaTools.getIntegerPart(wilsonCi(1)))
			val newReferenceValueLow = tmpReference.valueLow :+ newDist(javatools.JavaTools.getIntegerPart(wilsonCi(0)))
			
			reference.valueHi = newReferenceValueHi
			reference.valueLow = newReferenceValueLow
			reference.valueMedian = newReferenceValueMedian
			
		} //Case : 2
		else if (reference.valueMedian.size == 3) {
			
			val newReferenceValueMedian1 = tmpReference.valueMedian :+ medianCalculator(tmpReference.valueMedian)
			val newReferenceValueHi1 = tmpReference.valueHi :+ medianCalculator(tmpReference.valueHi)
			val newReferenceValueLow1 = tmpReference.valueLow :+ medianCalculator(tmpReference.valueLow)
			
			reference.valueHi = newReferenceValueHi1
			reference.valueLow = newReferenceValueLow1
			reference.valueMedian = newReferenceValueMedian1
			
			val newReferenceValueMedian = reference.valueMedian.map(f => reference.valueMedian.last)
			reference.valueMedian = newReferenceValueMedian
			val newReferenceValueHi = reference.valueHi.map(f => reference.valueHi.last)
			reference.valueHi = newReferenceValueHi
			val newReferenceValueLow = reference.valueLow.map(f => reference.valueLow.last)
			reference.valueLow = newReferenceValueLow
		} //Case : 3
		else {
			
			val newReferenceValueMedian2 = tmpReference.valueMedian :+ (0.99 * tmpReference.valueMedian.last + 0.01 * current.valueMedian.last)
			val newReferenceValueHi2 = tmpReference.valueHi :+ (0.99 * tmpReference.valueHi.last + 0.01 * newDist(javatools.JavaTools.getIntegerPart(wilsonCi(1))))
			val newReferenceValueLow2 = tmpReference.valueLow :+ (0.99 * tmpReference.valueLow.last + 0.01 * newDist(javatools.JavaTools.getIntegerPart(wilsonCi(0))))
			reference.valueHi = newReferenceValueHi2
			reference.valueLow = newReferenceValueLow2
			reference.valueMedian = newReferenceValueMedian2 |\label{lsl-fincases}|
			
			//Anomalies dection : compare the current with the reference
			if ((BigDecimal(current.valueMedian.last) - BigDecimal(current.valueLow.last) > |\label{debutdetection}| reference.valueHi.last || current.valueMedian.last + current.valueHi.last < reference.valueLow.last) && scala.math.abs(current.valueMedian.last - reference.valueMedian.last) > 1) {
				
				val updateAlarmsDates = alarmsDates.dates :+ date
				alarmsDates.dates = updateAlarmsDates
				
				val updateAlarmsValues = alarmsValues.medians :+ current.valueMedian.last
				alarmsValues.medians = updateAlarmsValues
			}
		}
	}
}
\end{lstlisting}

 Nous résumons le fonctionnement de la méthode \textit{findAlarms} dans les étapes suivantes. Notons que \textit{spark}  est l'objet SparkSession créé au début du programme Spark,  \textit{date} est la période courante, \textit{reference} est l'état référence du lien, \textit{dataPeriod} est la liste des RTTs différentiels de la période  \textit{date}, \textit{current} est l'état courant du lien, \textit{alarmsDates} sont les dates d'alarmes, \textit{alarmsValues} sont les RTTs différentiels médians et \textit{dates} sont les dates où on constate une distribution assez représentable des RTTs différentiels.   
 
 \begin{enumerate}
 	\item trouver les indices,  dans \textit{dataPeriod.bins}, correspondants à la période \textit{date} (\textit{indices}) (ligne \ref{lst:lineindices});
 	
 	\item trouver les RTTs différentiels, dans \textit{dataPeriod.rttDiffs}, identifiés durant  \textit{date} (\textit{dist}) (ligne \ref{lst:linedist});
 	
 	\item la représentativité d'une distribution dépend d'une valeur donnée  (ici $3$), Si la taille de \textit{dist} est supérieure à cette valeur, nous ajoutons \textit{date} à \textit{dates} et nous continuons l'analyse de cette période (ligne \ref{lst:linedistSizesup});
 	\item calculer le score de Wilson, qui fournit deux valeurs, et multiplier ce score par la taille de la distribution  (ligne \ref{lsl:linewilson});
 	\item calculer l'état courant du lien (\textit{current}), cela inclut la borne inférieure de l'intervalle de confiance de la médiane des RTTs différentiels, le RTT différentiel médian et la borne supérieure de l'intervalle de confiance de la médiane des RTTs différentiels (ligne \ref{lst:lineupdateLinkCurrentState});
 	\item ordonner les RTTs différentiels (\textit{dist}) (ligne \ref{linenewDist});
 	\item récupérer l'état référence du lien (ligne \ref{line-getreference});
 	\item mettre à jour l'état référence du lien \textit{reference} suivant les trois cas (lignes entre \ref{lsl-debutcases} et \ref{lsl-fincases});
 	\item  comparer les deux intervalles de confiance s'il s'agit du cas $3$ et mettre à jour \textit{alarmsValues} et \textit{alarmsDates} si une anomalie est identifiée (ligne \ref{debutdetection}).
 	
 \end{enumerate}



\subsubsection{Exécution d'une application Spark} 

Afin de pouvoir exécuter une application Spark, il faut qu'elle soit packagée dans un fichier de type JAR. Ce dernier doit reprendre une classe contenant une méthode \textit{main} et doit reprendre toutes les dépendances nécessaires à l'exécution de l'application.  Enfin, l'application Spark  est soumise avec la commande \textit{bin/spark-submit}. Tout en indiquant les paramètres du \textit{cluster} et les paramètre de l'application. Nous donnons deux exemples de soumission. La première est destinée à être exécutée dans une machine locale. Alors que la deuxième soumission est destinée à être executée dans un \textit{cluster} de machines. 

\paragraph{Mode local}~

\begin{lstlisting}[language=bash,firstnumber=1, caption={Exemple de la soumissions d'un traitement sur Spark},label={lst:submit}, basicstyle = \small,escapechar=|,numbers=left,
stepnumber=1]
~$ bin/spark-submit --class ripeatlasanalysis.AnalyseTraceroute     --master local --driver-memory 30G  --conf "spark.network.timeout=10000000" DelayAnalysis-0.0.5-SNAPSHOT-jar-with-dependencies.jar  1517961600  1518134400 3600 
\end{lstlisting}
La commande \textit{bin/spark-submit} prend plusieurs paramètres, nous présentons quelques paramètres utilisés :
\begin{itemize}
	\item \textit{class} est un objet Scala contenant la fonction \textit{main};
	\item  \textit{master} est l'URL du \textit{cluster} (voir les différents modes dans la section \ref{spark-master-modes});
	\item \textit{driver-memory} est la mémoire dont le processus du \textit{driver} peut utiliser;
	
	\item  \textit{--conf} "key = value" est une manière de configurer l'application Spark. Dans l'exemple, "spark.network.timeout=10000000", $ 10000000 $ est le temps durant lequel le \textit{driver} doit recevoir  des mises à jour de la part des différents workers; après ce temps, le worker n'est plus considéré comme actif.
\end{itemize}

\paragraph{Amazon EMR }~

\begin{lstlisting}[language=bash,firstnumber=1, caption={Exemple de la soumissions d'un traitement sur Spark},label={lst:submitemr}, basicstyle = \small,escapechar=|,numbers=left,
stepnumber=1]
spark-submit --deploy-mode cluster --class ripeatlasanalysis.traceroutesAnalysis s3://amazon-emr-bucket/DelayAnalysis-AWS-EMR-Final-0.0.2-SNAPSHOT-jar-with-dependencies.jar 1517961600 1518048000 3600 s3://ripeatlasdata/traceroute/source=api/af_=4/type_=builtin/msm=5004/year=2018/month=2/day=07 s3://amazon-emr-bucket/resultats
\end{lstlisting}
\paragraph{Résultats finaux}~

Nous sauvegardons le résultat de l'analyse dans un fichier JSON en vue de toute réutilisation. Un exemple de résultat est illustré dans le Listing \ref{resultLink}.

\begin{lstlisting}[language=json,firstnumber=1, caption={Exemple des résultats de l'analyse d'un lien}, label=resultLink]
{
	"link": {
		"ip1": "185.147.12.31",
		"ip2": "89.105.200.57"
	},
	"reference": {
		"valueMedian": [
		2.991,
		2.991,
		2.991,
		2.991,
		13.67572,
		13.5727878
		],
		"valueHi": [
		4.402,
		4.402,
		4.402,
		4.402,
		15.352609999999999,
		15.243713899999998
		],
		"valueLow": [
		2.394,
		2.394,
		2.394,
		2.394,
		12.83469,
		12.7333531
		],
		"valueMean": []
	},
	"current": {
		"valueMedian": [
		2.991,
		3.081,
		2.9109999999999996,
		823.963,
		1071.463,
		3.3825000000000003
		],
		"valueHi": [
		0.472,
		1.321,
		1.7240000000000004,
		175.5,
		28,
		1.0804999999999998
		],
		"valueLow": [
		10.741,
		0.39,
		0.5169999999999996,
		142.5,
		25,
		0.6815000000000003
		],
		"valueMean": []
	},
	"alarmsDates": [
	1514784200,
	1514787800
	],
	"alarmsValues": [
	1071.463,
	3.3825000000000003
	],
	"dates": [
	1514769800,
	1514773400,
	1514777000,
	1514780600,
	1514784200,
	1514787800
	]
}
\end{lstlisting}






%Les fonctionnalités de Spark sont accessibles avec les  APIs en Scala, Java et Python. Nous avons choisi l'utilisation de l'API en Scala parce que Scala est le langage natif de Spark. De plus, Scala est interopérable avec Java. 


%décrit dans la section \ref{apache-spark}, en utilisant l'API Scala (voir la présentation de Scala dans \ref{scala-presentation}). 
%L'outil implémenté est décrit dans le chapitre \ref{chap:algorith-detection}. 


%[!] Avec Spark, on travaille sur des collections d'objets, sur lesquelles on applique des traitements. Dans une application écrite en Spark, nous avons besoin des classes modélisant les objets tout au long de l'analyse de données, appelées \textit{case class}. De plus, nous avons besoin de définir les fonctions à appliquer  sur ces objets. Nous avons décrit les différentes \textit{case class} conçues pour traiter les objets traceroutes, dans l'annexe \ref{application:spark}, en vue de tracer l'évolution des RTTs différentiels des liens au cours du temps. 




\section{Conclusion}

Dans ce chapitre, nous avons décrit l'implémentation de l'outil de détection en utilisant trois technologies différentes dédiées à la manipulation des données massives.  La première représente l'implémentation du travail de référence. La deuxième  vise la modification de l'implémentation de référence afin d'utiliser une autre technologie de stockage de données. Tandis que la troisième est une reproduction de l'implémentation de référence avec un framework dédié au traitement des données massives.
%A travers cette implémentation, nous avons évalué les différentes défis relatifs à la réutilisation du code source des autres et la façon de réutiliser impélementation destinée à être distribuée.
Dans  le chapitre \ref{chap:application-on-traceroutes}, nous allons évaluer l'application de ces implémentations sur différents échantillons de traceroutes.
% dans le chapitre \ref{chap:application-on-traceroutes} avec d'autres implémentations utilisant d'autres technologies Big Data. 