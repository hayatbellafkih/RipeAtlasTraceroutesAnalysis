\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
%Rappel de la problématique.

L'objectif du présent travail consiste à évaluer un sous-ensemble de technologies Big Data sur des données à grande échelle en provenance du dépôt d'Atlas. Etant donné que l'évaluation des technologies Big Data peut prendre plusieurs formes, la présente évaluation envisage 
 la mise en place de la technologie  et   le calcul du temps d'exécution obtenu en analysant différents échantillons de données avec cette technologie.
 
 Le modèle choisi pour l'évaluation des technologies Big Data se base sur des réponses aux requêtes de type traceroute effectués par des sondes.  Le volume et la vélocité  avec lesquels les traceroutes  sont générés dépassent la capacité des outils traditionnels comme les bases de données relationnelles. La gestion des données à grande échelle  fait partie des objectifs du Big Data.
%L'évaluation de la convenance d'une technologie Big Data nécessite de disposer de données assez suffisantes pour couvrir  le plus des  cas possibles. 
%Nous avons choisi des données dans le domaine des réseaux informatiques,  disponibles sur le dépôt de RIPE Atlas,  d'où l'intérêt  de bien détailler ce projet.



%Que peut-on déduire du travail de recherche ?

%Résultats de recherche et réponses aux questions de recherche.
%% reponse à la question 1 de recherche : pourquoi passer au big data


%données   ayant un volume important.  Une dizaine de gigaoctets de traceroutes sont générés quotidiennement. De plus, les données quotidiennes augmentent d'un jour au un autre.
%ont démontré  les limites des outils traditionnels et leur incapacité de  manipuler les données à grande échelle.

 
Après avoir manipulé de nombreuses traceroutes, nous avons appris que les technologies Big Data doivent garantir plusieurs tâches dans un processus d'analyse de donnés. En particulier, la collecte de données, le stockage de données, le traitement de ces dernières, voire d'autres.

 %sont capables de gérer les données à grande échelle. 
%Les technologies Big Data sont capables de manipuler les données à grande échelle
 En terme de stockage,   le service Amazon S3 est dédié au stockage des fichiers de données de grande taille. Les données stockées dans ces fichiers sont accessibles à travers le service Amazon Athena. 
  Une autre catégorie de stockage est  les bases de données NoSQL, dont l'exemple  du service Amazon DynamoDB. Ce dernier   garantit le stockage de toute quantité de données. Un autre exemple est la base   de données NoSQL   MongoDB Atlas. Du fait qu'elle est de type document, elle peut stocker les réponses de requête traceroute sauvegardées en format JSON.  
  
  
  En plus du stockage de données massives, la récupération de ces données pour  les traiter  rapidement est un autre défi du Big Data.
  C'est pourquoi, l'utilisation d'une technologie permettant de distribuer ces traitements permet d'améliorer considérablement une analyse en terme de temps. C'est l'objectif  du framework Spark, tel qu'il garantit la  distribution des traitements des données sur un \textit{cluster} de machines.
  
   Chaque technologie Big Data apporte ses propres concepts. En effet, pour un traitement donné, des solutions sont proposées en adéquation avec ces concepts. Une base de données MongoDB manipule des collections, des documents, etc. Amazon Athena adopte le principe de \textit{schema on read}, l'organisation des fichiers de données à travers le partitionnement, etc. La mise en place d'une application basée sur Spark nécessite  la traduction des traitements pour qu'elles soient effectués le plus possible  de manière distribuée. De plus, il  faut construire les machines du \textit{cluster} tout en prenant en compte le coût, le temps et les volumes de données manipulés.
    
   %demande des efforts pour choisir les entités du cluster et  configurer ce dernier tout en prenant en compte le volume de données traitées, la nature des opérations, les caractéristiques techniques du cluster, etc.  Tous ces éléments ont un effet sur les frais d'utilisation de la présente technologie.
  
  
  % gérer différents types de projets vu les modules riches qu'il inclut. Comme les projets où on manipule des données semi-structurées, des données structurées, etc. Des projets relatifs au machine learning.  Ce framework est capable de distribuer les traitements sur cluster de machine et les données sont manipulées en mémoire.
 

%la réutilisation d'un travail existant
%Le travail de référence sur lequel ce travail est  
%Le travail de référence 
 %La réécriture complète ou partielle d'un travail autre travail peut 
%Les implémentations présentées dans ce travail utilise le principe de détection des anomalies créé par Fontugne et al. \cite{DBLP:journals/corr/FontugneAPB16}. La réécriture exacte n'est pas toujours optimale, car certains choix ont été faits selon la technologie avec laquelle est implémentée. 


%l'utilisation des technologies Big Data
%Suite à l'application  du sous-ensemble de technologies sur l'outil de détection, on distingue deux défis. Le premier est relatif à l'utilisation d'un modèle existant. Tandis que le deuxième défi est lié à la mise en place d'une technologie Big Data.


L'adoption d'une technologie Big Data en particulier pour traiter des données  doit prendre en compte plusieurs entrées. Nous citons quelques-unes selon l'évaluation réalisée dans ce mémoire. D'abord, il faut préciser la fréquence de l'analyse : si l'analyse est effectuée une seule fois ou d'une manière périodique. La fréquence de la génération des données à analyser.   Le temps admissible pour analyser un volume de données, au-delà de ce temps, l'analyse est considérée non optimale. 
%Ce temps dépend à la fois des données analysées et les choix techniques comme les caractéristiques techniques d'un cluster EMR. De plus, la nature de données manipulées est importante. 
%Ceci  permet de choisir la technologie la plus adaptée. Car 
La nature des données traitées, car certaines technologies sont conçues  et optimisées pour traiter un type de donnée en particulier, c'est l'exemple des bases de données NoSQL. La nature des traitements à appliquer sur les données. Les frais d'utilisation d'une technologie est un autre facteur dans le choix de cette dernière. 


%perspectives


%La disponibilité des outils informatiques permettant de stocker et de traiter des données à grande échelle, avec efficacité,  est important. Toutefois,  le modèle qui dirige l'ensemble de données est aussi de même degré d'importance dans un processus d'analyse de données, comme le cas  du modèle créé par Fontugne et al. \cite{DBLP:journals/corr/FontugneAPB16} pour la détection des anomalies.  Nous avons compris le fonctionnement ainsi que le paramétrage du modèle. Comme continuité du présent travail, on note quelques possibilités. 
Nous avons validé les résultats obtenus   avec  l'implémentation de référence avec ceux obtenus avec nos deux implémentations : les services d'Amazon S3 et Athena et  Spark/Scala. Notre évaluation des technologies Big Data a examiné la mise en place de ces dernières ainsi que  le temps d'exécution obtenu sur différents échantillons de données.  Comme  suite de ce travail, il est intéressant d'évaluer la précision de l'outil de détection  en choisissant des données ciblées.
Du fait que RIPE Atlas dispose du mode Streaming dans lequel les données peuvent être récupérées en temps réel, il est intéressant d'évaluer l'intégration de l'extension \textit{Spark streaming} pour analyser des données  d'Atlas en temps réel.  

 %soutil de détection 
%Par exemple, il est possible de réévaluer la précision  de ce modèle avec de nouvelles données, varier les paramètres du modèle de la détection comme la méthode adoptée au calcul des intervalles de confiance, etc. 


 






























